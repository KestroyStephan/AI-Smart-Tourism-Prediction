{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Adjust path to current directory if files are not in a 'data' subfolder\n",
        "DATA_DIR = Path(\".\")\n",
        "\n",
        "# Load files\n",
        "survey = pd.read_csv(DATA_DIR / \"FinalYearProject_Survey.csv\", low_memory=False)\n",
        "tripadvisor = pd.read_csv(DATA_DIR / \"tripadvisor_hotel_reviews.csv\", low_memory=False)\n",
        "\n",
        "# FIX 1: Add encoding='latin1' to handle special characters\n",
        "reviews = pd.read_csv(DATA_DIR / \"Reviews.csv\", low_memory=False, encoding='latin1')\n",
        "\n",
        "travel_agents = pd.read_csv(DATA_DIR / \"sri_lanka_travel_agents.csv\", low_memory=False)\n",
        "tourist_shops = pd.read_csv(DATA_DIR / \"sri_lanka_tourist_shops.csv\", low_memory=False)\n",
        "\n",
        "# FIX 2: Correct filename (matches your uploaded file) - Corrected typo from 'ri_lanka' to 'sri_lanka'\n",
        "aggregated = pd.read_csv(DATA_DIR / \"sri_lanka_aggregated_2020_2022.csv\", low_memory=False)\n",
        "\n",
        "enriched40 = pd.read_csv(DATA_DIR / \"Tourism_dataset.csv\", low_memory=False)\n",
        "\n",
        "# Quick survey counts\n",
        "total_responses = len(survey)\n",
        "\n",
        "# FIX 3: Use the exact column name from the CSV\n",
        "country_col = '2. What is your country or nationality?'\n",
        "\n",
        "domestic = survey[survey[country_col].str.contains(\"Sri Lanka|Sri Lankan|Srilanka|Sinhal\", case=False, na=False)]\n",
        "international = survey[~survey.index.isin(domestic.index)]\n",
        "\n",
        "print(\"Total responses:\", total_responses)\n",
        "print(\"Domestic responses:\", len(domestic))\n",
        "print(\"International responses:\", len(international))\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    \"total_responses\": total_responses,\n",
        "    \"domestic\": len(domestic),\n",
        "    \"international\": len(international)\n",
        "}\n",
        "pd.Series(summary).to_csv(DATA_DIR / \"survey_summary_counts.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1QpZTz8JGzd",
        "outputId": "1f445877-1632-4354-ce7b-bc37adaeac52"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total responses: 101\n",
            "Domestic responses: 61\n",
            "International responses: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\".\")\n",
        "\n",
        "# FIX 1: Add encoding='latin1' to avoid UnicodeDecodeError in Reviews.csv\n",
        "reviews = pd.read_csv(DATA_DIR / \"Reviews.csv\", low_memory=False, encoding='latin1')\n",
        "tripadvisor = pd.read_csv(DATA_DIR / \"tripadvisor_hotel_reviews.csv\", low_memory=False)\n",
        "enriched40 = pd.read_csv(DATA_DIR / \"Tourism_dataset.csv\", low_memory=False)\n",
        "\n",
        "def standardize(df, name_cols):\n",
        "    # Attempt to pick a column for place name\n",
        "    for c in name_cols:\n",
        "        if c in df.columns:\n",
        "            df = df.rename(columns={c: \"place_name\"})\n",
        "            break\n",
        "\n",
        "    # Find lat/lon columns\n",
        "    lat_cols = [c for c in df.columns if \"lat\" in c.lower()]\n",
        "    lon_cols = [c for c in df.columns if \"lon\" in c.lower() or \"lng\" in c.lower()]\n",
        "\n",
        "    if lat_cols:\n",
        "        df = df.rename(columns={lat_cols[0]: \"latitude\"})\n",
        "    if lon_cols:\n",
        "        df = df.rename(columns={lon_cols[0]: \"longitude\"})\n",
        "    return df\n",
        "\n",
        "# FIX 2: Added correct column names based on your files\n",
        "tripadvisor = standardize(tripadvisor, name_cols=[\"Destination\", \"name\", \"place\", \"hotel_name\"])\n",
        "reviews = standardize(reviews, name_cols=[\"Location_Name\", \"Location\", \"place\", \"location\", \"name\"])\n",
        "enriched40 = standardize(enriched40, name_cols=[\"name\", \"place_name\"])\n",
        "\n",
        "# Quick fix: trim place names\n",
        "for df in (tripadvisor, reviews, enriched40):\n",
        "    if \"place_name\" in df.columns:\n",
        "        df[\"place_name\"] = df[\"place_name\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True).str.lower()\n",
        "\n",
        "print(tripadvisor.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWBDsmyoKFGs",
        "outputId": "280f2640-eef1-423b-95e1-ab9ce3ed8989"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                place_name District      Timespan  \\\n",
            "0  attidiya bird sanctuary  colombo  3 months ago   \n",
            "1  attidiya bird sanctuary  colombo  3 months ago   \n",
            "2  attidiya bird sanctuary  colombo   5 years ago   \n",
            "\n",
            "                                              Review  \n",
            "0  spots scenic make ideal dwelling birds creatur...  \n",
            "1      good place birdwatching different type around  \n",
            "2  calm peaceful location visit time got separate...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import difflib\n",
        "\n",
        "DATA_DIR = Path(\".\")\n",
        "\n",
        "# 1. Load files\n",
        "tripadvisor = pd.read_csv(DATA_DIR / \"tripadvisor_hotel_reviews.csv\", low_memory=False)\n",
        "enriched40 = pd.read_csv(DATA_DIR / \"Tourism_dataset.csv\", low_memory=False)\n",
        "reviews = pd.read_csv(DATA_DIR / \"Reviews.csv\", low_memory=False, encoding='latin1')\n",
        "\n",
        "# 2. Standardize column names (ensure 'place_name' exists)\n",
        "def standardize(df, name_cols):\n",
        "    for c in name_cols:\n",
        "        if c in df.columns:\n",
        "            df = df.rename(columns={c: \"place_name\"})\n",
        "            return df\n",
        "    return df\n",
        "\n",
        "tripadvisor = standardize(tripadvisor, [\"Destination\", \"name\", \"place\", \"hotel_name\"])\n",
        "enriched40 = standardize(enriched40, [\"name\", \"place_name\"])\n",
        "reviews = standardize(reviews, [\"Location_Name\", \"Location\", \"place\", \"location\", \"name\"])\n",
        "\n",
        "# 3. Clean strings (lowercase, strip)\n",
        "def clean_names(df, name_col=\"place_name\"):\n",
        "    if name_col in df.columns:\n",
        "        df[name_col] = df[name_col].astype(str).str.strip().str.lower()\n",
        "    return df\n",
        "\n",
        "tripadvisor = clean_names(tripadvisor)\n",
        "enriched40 = clean_names(enriched40)\n",
        "reviews = clean_names(reviews)\n",
        "\n",
        "# 4. Merge Logic\n",
        "canonical_names = enriched40[\"place_name\"].dropna().unique().tolist()\n",
        "\n",
        "# Exact Match\n",
        "tripadvisor_exact = tripadvisor.merge(enriched40, on=\"place_name\", how=\"left\", suffixes=(\"\", \"_enr\"))\n",
        "\n",
        "# Identify missing matches\n",
        "missing_mask = tripadvisor_exact[\"category\"].isna()\n",
        "print(f\"Records missing exact match: {missing_mask.sum()}\")\n",
        "\n",
        "# Fuzzy Match Function using difflib\n",
        "def get_best_match(name, choices, cutoff=0.75):\n",
        "    matches = difflib.get_close_matches(name, choices, n=1, cutoff=cutoff)\n",
        "    return matches[0] if matches else None\n",
        "\n",
        "# Apply fuzzy matching\n",
        "updates = []\n",
        "for idx, row in tripadvisor_exact[missing_mask].iterrows():\n",
        "    name = row[\"place_name\"]\n",
        "    if pd.isna(name) or name == \"\":\n",
        "        continue\n",
        "\n",
        "    match = get_best_match(name, canonical_names, cutoff=0.75)\n",
        "    if match:\n",
        "        updates.append((idx, match))\n",
        "\n",
        "print(f\"Found {len(updates)} fuzzy matches.\")\n",
        "\n",
        "# Update the DataFrame with fuzzy results\n",
        "for idx, match_name in updates:\n",
        "    enr_row = enriched40[enriched40[\"place_name\"] == match_name].iloc[0]\n",
        "    for col in enriched40.columns:\n",
        "        tripadvisor_exact.at[idx, col] = enr_row[col]\n",
        "\n",
        "# Save result\n",
        "output_file = DATA_DIR / \"tripadvisor_enriched_merged.csv\"\n",
        "tripadvisor_exact.to_csv(output_file, index=False)\n",
        "print(f\"Merged tripadvisor saved to: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JReHMTH2KPde",
        "outputId": "fc0b3ae1-37fb-418c-87f2-58f17b4598e0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Records missing exact match: 33219\n",
            "Found 1507 fuzzy matches.\n",
            "Merged tripadvisor saved to: tripadvisor_enriched_merged.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from textblob import TextBlob\n",
        "import difflib\n",
        "\n",
        "DATA_DIR = Path(\".\")\n",
        "\n",
        "# 1. Load Data\n",
        "# Canonical list (enriched40)\n",
        "enriched40 = pd.read_csv(DATA_DIR / \"Tourism_dataset.csv\", low_memory=False)\n",
        "\n",
        "# TripAdvisor (merged in step 3) - Has Text, No Rating\n",
        "tripadvisor_enriched = pd.read_csv(DATA_DIR / \"tripadvisor_enriched_merged.csv\", low_memory=False)\n",
        "\n",
        "# Reviews (raw) - Has Ratings\n",
        "reviews = pd.read_csv(DATA_DIR / \"Reviews.csv\", low_memory=False, encoding='latin1')\n",
        "\n",
        "# 2. Standardize Reviews.csv to match canonical place_names\n",
        "if \"Location_Name\" in reviews.columns:\n",
        "    reviews = reviews.rename(columns={\"Location_Name\": \"place_name\"})\n",
        "\n",
        "# Standardize strings\n",
        "reviews[\"place_name\"] = reviews[\"place_name\"].astype(str).str.strip().str.lower()\n",
        "enriched40[\"place_name\"] = enriched40[\"name\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "# Fuzzy Match Reviews to Canonical List\n",
        "canonical_names = enriched40[\"place_name\"].dropna().unique().tolist()\n",
        "\n",
        "# Exact match helper\n",
        "reviews = reviews.merge(enriched40[[\"place_name\"]], on=\"place_name\", how=\"left\", indicator=\"matched\")\n",
        "\n",
        "# Find unmatched and try fuzzy matching\n",
        "unmatched_mask = reviews[\"matched\"] == \"left_only\"\n",
        "unmatched_names = reviews.loc[unmatched_mask, \"place_name\"].unique()\n",
        "match_map = {}\n",
        "\n",
        "for name in unmatched_names:\n",
        "    matches = difflib.get_close_matches(name, canonical_names, n=1, cutoff=0.75)\n",
        "    if matches:\n",
        "        match_map[name] = matches[0]\n",
        "\n",
        "# Apply corrections\n",
        "reviews.loc[unmatched_mask, \"place_name\"] = reviews.loc[unmatched_mask, \"place_name\"].map(match_map).fillna(reviews.loc[unmatched_mask, \"place_name\"])\n",
        "\n",
        "# 3. Aggregate Ratings (from Reviews.csv)\n",
        "agg_reviews = reviews.groupby(\"place_name\").agg(\n",
        "    avg_rating=(\"Rating\", \"mean\"),\n",
        "    review_count=(\"Rating\", \"count\")\n",
        ").reset_index()\n",
        "\n",
        "# 4. Aggregate Sentiment (from TripAdvisor)\n",
        "def get_sentiment(text):\n",
        "    try:\n",
        "        return TextBlob(str(text)).sentiment.polarity\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "if \"Review\" in tripadvisor_enriched.columns:\n",
        "    tripadvisor_enriched[\"sentiment\"] = tripadvisor_enriched[\"Review\"].apply(get_sentiment)\n",
        "    agg_sentiment = tripadvisor_enriched.groupby(\"place_name\").agg(\n",
        "        avg_sentiment=(\"sentiment\", \"mean\")\n",
        "    ).reset_index()\n",
        "else:\n",
        "    agg_sentiment = pd.DataFrame(columns=[\"place_name\", \"avg_sentiment\"])\n",
        "\n",
        "# 5. Merge everything into Master\n",
        "master = enriched40.copy()\n",
        "master = master.merge(agg_reviews, on=\"place_name\", how=\"left\")\n",
        "master = master.merge(agg_sentiment, on=\"place_name\", how=\"left\")\n",
        "\n",
        "# Fill NaNs\n",
        "global_avg_rating = agg_reviews[\"avg_rating\"].mean() if not agg_reviews.empty else 0\n",
        "master[\"avg_rating\"] = master[\"avg_rating\"].fillna(global_avg_rating)\n",
        "master[\"review_count\"] = master[\"review_count\"].fillna(0)\n",
        "master[\"avg_sentiment\"] = master[\"avg_sentiment\"].fillna(0.0)\n",
        "\n",
        "# 6. Compute Empowerment Signal\n",
        "max_vendor = master[\"vendor_count\"].max()\n",
        "if pd.isna(max_vendor) or max_vendor == 0: max_vendor = 1\n",
        "\n",
        "max_reviews = master[\"review_count\"].max()\n",
        "if pd.isna(max_reviews) or max_reviews == 0: max_reviews = 1\n",
        "\n",
        "master[\"vendor_norm\"] = master[\"vendor_count\"] / max_vendor\n",
        "master[\"reviews_norm\"] = master[\"review_count\"] / max_reviews\n",
        "master[\"local_empowerment_index\"] = pd.to_numeric(master[\"local_empowerment_index\"], errors='coerce').fillna(0)\n",
        "\n",
        "# Final Formula\n",
        "master[\"local_empowerment_signal\"] = (\n",
        "    0.4 * master[\"vendor_norm\"] +\n",
        "    0.4 * master[\"reviews_norm\"] +\n",
        "    0.2 * (master[\"local_empowerment_index\"] / 10.0)\n",
        ")\n",
        "\n",
        "# 7. Save\n",
        "master.to_csv(DATA_DIR / \"master_enriched_dataset.csv\", index=False)\n",
        "print(\"Master enriched dataset saved:\", DATA_DIR / \"master_enriched_dataset.csv\")\n",
        "print(master[[\"place_name\", \"avg_rating\", \"local_empowerment_signal\"]].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llXyF42rLFMs",
        "outputId": "5d3d56e1-1eee-43ae-c43a-25691abffc5e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master enriched dataset saved: master_enriched_dataset.csv\n",
            "                    place_name  avg_rating  local_empowerment_signal\n",
            "0       sigiriya rock fortress    4.645614                  0.634872\n",
            "1                    ella rock    4.177612                  0.260000\n",
            "2                  adam's peak    4.177612                  0.300000\n",
            "3  horton plains national park    4.326425                  0.331966\n",
            "4  temple of the tooth (kandy)    4.177612                  0.500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DATA_DIR = Path(\".\")\n",
        "MODEL_DIR = Path(\"models\")\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(DATA_DIR / \"master_enriched_dataset.csv\")\n",
        "\n",
        "# Clean Target: category\n",
        "df = df.dropna(subset=[\"category\"])\n",
        "df[\"category\"] = df[\"category\"].str.lower().str.strip()\n",
        "\n",
        "# FIX: Correct typo if present (found in inspection: \"hPeradeniya eco\")\n",
        "df[\"category\"] = df[\"category\"].replace({\"hperadeniya eco\": \"eco\"})\n",
        "\n",
        "# Features\n",
        "features = [\n",
        "    \"eco_score\", \"cultural_score\", \"vendor_count\", \"local_empowerment_index\",\n",
        "    \"accessibility_score\", \"danger_level\", \"avg_rating\", \"review_count\", \"avg_sentiment\"\n",
        "]\n",
        "\n",
        "# Convert crowd_level to numeric\n",
        "df[\"crowd_level_num\"] = df[\"crowd_level\"].map({\"low\":1, \"medium\":2, \"high\":3})\n",
        "# Fill missing crowd_levels with median (2)\n",
        "df[\"crowd_level_num\"] = df[\"crowd_level_num\"].fillna(2)\n",
        "\n",
        "features.append(\"crowd_level_num\")\n",
        "\n",
        "# Fill missing numeric features\n",
        "for f in features:\n",
        "    df[f] = pd.to_numeric(df[f], errors=\"coerce\").fillna(df[f].median())\n",
        "\n",
        "X = df[features]\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[\"category\"])\n",
        "print(\"Classes identified:\", le.classes_)\n",
        "\n",
        "# Train-test split\n",
        "# Using stratify=y ensures we keep the same proportion of classes in train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Scale numerics\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "# Model 1: Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "# Reduced cv to 3 for small dataset\n",
        "rf_scores = cross_val_score(rf, X_train_s, y_train, cv=StratifiedKFold(3), scoring=\"accuracy\")\n",
        "print(f\"RF CV accuracy mean: {rf_scores.mean():.3f}\")\n",
        "\n",
        "rf.fit(X_train_s, y_train)\n",
        "rf_test_acc = rf.score(X_test_s, y_test)\n",
        "print(f\"RF test accuracy: {rf_test_acc:.3f}\")\n",
        "\n",
        "# Model 2: Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', random_state=42)\n",
        "lr_scores = cross_val_score(lr, X_train_s, y_train, cv=StratifiedKFold(3), scoring=\"accuracy\")\n",
        "print(f\"LR CV accuracy mean: {lr_scores.mean():.3f}\")\n",
        "\n",
        "lr.fit(X_train_s, y_train)\n",
        "lr_test_acc = lr.score(X_test_s, y_test)\n",
        "print(f\"LR test accuracy: {lr_test_acc:.3f}\")\n",
        "\n",
        "# Choose best\n",
        "if rf_test_acc >= lr_test_acc:\n",
        "    best_model = rf\n",
        "    best_name = \"random_forest\"\n",
        "    best_acc = rf_test_acc\n",
        "else:\n",
        "    best_model = lr\n",
        "    best_name = \"logistic_regression\"\n",
        "    best_acc = lr_test_acc\n",
        "\n",
        "# Save model + preprocessing\n",
        "joblib.dump(best_model, MODEL_DIR / \"best_model.pkl\")\n",
        "joblib.dump(scaler, MODEL_DIR / \"scaler.pkl\")\n",
        "joblib.dump(le, MODEL_DIR / \"label_encoder.pkl\")\n",
        "print(f\"Saved best model: {best_name} with test acc {best_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKuaAgKsLra-",
        "outputId": "5cf30a69-16a7-495d-cec7-ac4ef96306e2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes identified: ['cultural' 'eco' 'mixed']\n",
            "RF CV accuracy mean: 0.876\n",
            "RF test accuracy: 0.875\n",
            "LR CV accuracy mean: 0.939\n",
            "LR test accuracy: 0.750\n",
            "Saved best model: random_forest with test acc 0.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Adjust path to current directory\n",
        "DATA_DIR = Path(\".\")\n",
        "\n",
        "# Use the actual filename present in your environment\n",
        "fn = DATA_DIR / \"Tourism_dataset.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(fn, low_memory=False)\n",
        "\n",
        "    # Add missing columns with sensible defaults\n",
        "    defaults = {\n",
        "        \"eco_score\": 5,\n",
        "        \"cultural_score\": 5,\n",
        "        \"vendor_count\": 5,\n",
        "        \"local_empowerment_index\": 5,\n",
        "        \"accessibility_score\": 5,\n",
        "        \"crowd_level\": \"medium\",\n",
        "        \"danger_level\": 3\n",
        "    }\n",
        "\n",
        "    added_cols = []\n",
        "    for col, val in defaults.items():\n",
        "        if col not in df.columns:\n",
        "            df[col] = val\n",
        "            added_cols.append(col)\n",
        "\n",
        "    # Save only if changes were made or to ensure format\n",
        "    df.to_csv(fn, index=False)\n",
        "\n",
        "    if added_cols:\n",
        "        print(f\"Added columns: {added_cols}\")\n",
        "    else:\n",
        "        print(\"All columns already present. File verified.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find file {fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-QhopdKMc1N",
        "outputId": "fd984c49-3595-4e9a-d78e-02a702209d11"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All columns already present. File verified.\n"
          ]
        }
      ]
    }
  ]
}