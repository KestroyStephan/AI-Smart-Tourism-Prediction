{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1QpZTz8JGzd",
        "outputId": "1f445877-1632-4354-ce7b-bc37adaeac52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Data found in: D:\\Projects\\Projects Collection\\Final Year Project\\Tourism Application\\data\n",
            "Total responses: 101\n",
            "Domestic responses: 61\n",
            "International responses: 40\n",
            "âœ… Summary saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# --- ROBUST PATH DETECTION ---\n",
        "# We try multiple locations to find where 'data' is hiding\n",
        "possible_paths = [\n",
        "    Path(\"data\"),           # data/ inside current folder\n",
        "    Path(\"../data\"),        # data/ in parent folder (common for notebooks)\n",
        "    Path(\".\"),              # current folder\n",
        "    Path(\"D:/Projects/Projects Collection/Final Year Project/Tourism Application/data\") # Hardcoded fallback\n",
        "]\n",
        "\n",
        "DATA_DIR = None\n",
        "for path in possible_paths:\n",
        "    if (path / \"FinalYearProject_Survey.csv\").exists():\n",
        "        DATA_DIR = path\n",
        "        print(f\"âœ… Data found in: {DATA_DIR.resolve()}\")\n",
        "        break\n",
        "\n",
        "if DATA_DIR is None:\n",
        "    print(\"âŒ Error: Could not find 'FinalYearProject_Survey.csv'.\")\n",
        "    print(\"Checked locations:\", [p.resolve() for p in possible_paths])\n",
        "    # Stop execution if data isn't found\n",
        "    sys.exit(\"Stopping execution: Data not found.\")\n",
        "# -----------------------------\n",
        "\n",
        "# Load files\n",
        "try:\n",
        "    survey = pd.read_csv(DATA_DIR / \"FinalYearProject_Survey.csv\", low_memory=False)\n",
        "    tripadvisor = pd.read_csv(DATA_DIR / \"tripadvisor_hotel_reviews.csv\", low_memory=False)\n",
        "    # Encoding fix for Reviews.csv\n",
        "    reviews = pd.read_csv(DATA_DIR / \"Reviews.csv\", low_memory=False, encoding='latin1')\n",
        "    travel_agents = pd.read_csv(DATA_DIR / \"sri_lanka_travel_agents.csv\", low_memory=False)\n",
        "    tourist_shops = pd.read_csv(DATA_DIR / \"sri_lanka_tourist_shops.csv\", low_memory=False)\n",
        "    \n",
        "    # Handle potentially different filename\n",
        "    if (DATA_DIR / \"sri_lanka_aggregated_2020_2022.csv\").exists():\n",
        "        aggregated = pd.read_csv(DATA_DIR / \"sri_lanka_aggregated_2020_2022.csv\", low_memory=False)\n",
        "    else:\n",
        "        aggregated = pd.read_csv(DATA_DIR / \"ri_lanka_aggregated_2020_2022.csv\", low_memory=False)\n",
        "\n",
        "    enriched40 = pd.read_csv(DATA_DIR / \"Tourism_dataset.csv\", low_memory=False)\n",
        "\n",
        "    # Analysis\n",
        "    total_responses = len(survey)\n",
        "    \n",
        "    # Flexible column search for 'Nationality'\n",
        "    country_col = '2. What is your country or nationality?'\n",
        "    if country_col not in survey.columns:\n",
        "        cols = [c for c in survey.columns if \"country\" in c.lower()]\n",
        "        if cols: country_col = cols[0]\n",
        "\n",
        "    domestic = survey[survey[country_col].str.contains(\"Sri Lanka|Sri Lankan|Srilanka|Sinhal\", case=False, na=False)]\n",
        "    international = survey[~survey.index.isin(domestic.index)]\n",
        "\n",
        "    print(\"Total responses:\", total_responses)\n",
        "    print(\"Domestic responses:\", len(domestic))\n",
        "    print(\"International responses:\", len(international))\n",
        "\n",
        "    # Save summary\n",
        "    summary = {\n",
        "        \"total_responses\": total_responses,\n",
        "        \"domestic\": len(domestic),\n",
        "        \"international\": len(international)\n",
        "    }\n",
        "    pd.Series(summary).to_csv(DATA_DIR / \"survey_summary_counts.csv\")\n",
        "    print(\"âœ… Summary saved successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ An error occurred during processing: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWBDsmyoKFGs",
        "outputId": "280f2640-eef1-423b-95e1-ab9ce3ed8989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Files loaded and columns standardized.\n",
            "TripAdvisor Head:\n",
            "                 place_name District      Timespan  \\\n",
            "0  attidiya bird sanctuary  colombo  3 months ago   \n",
            "1  attidiya bird sanctuary  colombo  3 months ago   \n",
            "2  attidiya bird sanctuary  colombo   5 years ago   \n",
            "\n",
            "                                              Review  \n",
            "0  spots scenic make ideal dwelling birds creatur...  \n",
            "1      good place birdwatching different type around  \n",
            "2  calm peaceful location visit time got separate...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# --- ROBUST PATH DETECTION ---\n",
        "possible_paths = [Path(\"data\"), Path(\"../data\"), Path(\".\")]\n",
        "DATA_DIR = None\n",
        "for path in possible_paths:\n",
        "    if (path / \"Tourism_dataset.csv\").exists():\n",
        "        DATA_DIR = path\n",
        "        break\n",
        "\n",
        "if DATA_DIR is None:\n",
        "    sys.exit(\"âŒ Error: Could not find 'Tourism_dataset.csv'. Check your data folder.\")\n",
        "# -----------------------------\n",
        "\n",
        "# Reload files\n",
        "tripadvisor = pd.read_csv(DATA_DIR / \"tripadvisor_hotel_reviews.csv\", low_memory=False)\n",
        "enriched40 = pd.read_csv(DATA_DIR / \"Tourism_dataset.csv\", low_memory=False)\n",
        "reviews = pd.read_csv(DATA_DIR / \"Reviews.csv\", low_memory=False, encoding='latin1')\n",
        "\n",
        "def standardize(df, name_cols):\n",
        "    for c in name_cols:\n",
        "        if c in df.columns:\n",
        "            df = df.rename(columns={c: \"place_name\"})\n",
        "            break\n",
        "    \n",
        "    lat_cols = [c for c in df.columns if \"lat\" in c.lower()]\n",
        "    lon_cols = [c for c in df.columns if \"lon\" in c.lower() or \"lng\" in c.lower()]\n",
        "    if lat_cols: df = df.rename(columns={lat_cols[0]: \"latitude\"})\n",
        "    if lon_cols: df = df.rename(columns={lon_cols[0]: \"longitude\"})\n",
        "    return df\n",
        "\n",
        "tripadvisor = standardize(tripadvisor, name_cols=[\"Destination\", \"name\", \"place\", \"hotel_name\"])\n",
        "reviews = standardize(reviews, name_cols=[\"Location_Name\", \"Location\", \"place\", \"location\", \"name\"])\n",
        "enriched40 = standardize(enriched40, name_cols=[\"name\", \"place_name\"])\n",
        "\n",
        "# Clean place names\n",
        "for df in (tripadvisor, reviews, enriched40):\n",
        "    if \"place_name\" in df.columns:\n",
        "        df[\"place_name\"] = df[\"place_name\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True).str.lower()\n",
        "\n",
        "print(\"âœ… Files loaded and columns standardized.\")\n",
        "print(\"TripAdvisor Head:\\n\", tripadvisor.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JReHMTH2KPde",
        "outputId": "fc0b3ae1-37fb-418c-87f2-58f17b4598e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records missing match: 33219\n",
            "Found 1507 fuzzy matches.\n",
            "âœ… Merged tripadvisor saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import difflib\n",
        "import sys\n",
        "\n",
        "# --- ROBUST PATH DETECTION ---\n",
        "possible_paths = [Path(\"data\"), Path(\"../data\"), Path(\".\")]\n",
        "DATA_DIR = None\n",
        "for path in possible_paths:\n",
        "    if (path / \"Tourism_dataset.csv\").exists():\n",
        "        DATA_DIR = path\n",
        "        break\n",
        "\n",
        "if DATA_DIR is None:\n",
        "    sys.exit(\"âŒ Error: Could not find data files.\")\n",
        "# -----------------------------\n",
        "\n",
        "# Load files\n",
        "tripadvisor = pd.read_csv(DATA_DIR / \"tripadvisor_hotel_reviews.csv\", low_memory=False)\n",
        "enriched40 = pd.read_csv(DATA_DIR / \"Tourism_dataset.csv\", low_memory=False)\n",
        "reviews = pd.read_csv(DATA_DIR / \"Reviews.csv\", low_memory=False, encoding='latin1')\n",
        "\n",
        "def standardize_names(df, possible_names):\n",
        "    for c in possible_names:\n",
        "        if c in df.columns:\n",
        "            df = df.rename(columns={c: \"place_name\"})\n",
        "            break\n",
        "    if \"place_name\" in df.columns:\n",
        "        df[\"place_name\"] = df[\"place_name\"].astype(str).str.strip().str.lower()\n",
        "    return df\n",
        "\n",
        "tripadvisor = standardize_names(tripadvisor, [\"Destination\", \"name\"])\n",
        "enriched40 = standardize_names(enriched40, [\"name\", \"place_name\"])\n",
        "\n",
        "# Canonical names\n",
        "canonical_names = enriched40[\"place_name\"].dropna().unique().tolist()\n",
        "\n",
        "# 1. Exact join\n",
        "tripadvisor_exact = tripadvisor.merge(enriched40, on=\"place_name\", how=\"left\", suffixes=(\"\",\"_enr\"))\n",
        "\n",
        "# 2. Fuzzy match\n",
        "missing_mask = tripadvisor_exact[\"category\"].isna()\n",
        "print(f\"Records missing match: {missing_mask.sum()}\")\n",
        "\n",
        "def get_best_match(name, choices, cutoff=0.75):\n",
        "    matches = difflib.get_close_matches(name, choices, n=1, cutoff=cutoff)\n",
        "    return matches[0] if matches else None\n",
        "\n",
        "updates = []\n",
        "for idx, row in tripadvisor_exact[missing_mask].iterrows():\n",
        "    name = row[\"place_name\"]\n",
        "    if pd.isna(name) or name == \"\": continue\n",
        "    match = get_best_match(name, canonical_names)\n",
        "    if match:\n",
        "        updates.append((idx, match))\n",
        "\n",
        "print(f\"Found {len(updates)} fuzzy matches.\")\n",
        "\n",
        "for idx, match_name in updates:\n",
        "    enr_row = enriched40[enriched40[\"place_name\"] == match_name].iloc[0]\n",
        "    for col in enriched40.columns:\n",
        "        tripadvisor_exact.at[idx, col] = enr_row[col]\n",
        "\n",
        "tripadvisor_exact.to_csv(DATA_DIR / \"tripadvisor_enriched_merged.csv\", index=False)\n",
        "print(\"âœ… Merged tripadvisor saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llXyF42rLFMs",
        "outputId": "5d3d56e1-1eee-43ae-c43a-25691abffc5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Master dataset saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from textblob import TextBlob\n",
        "import difflib\n",
        "import sys\n",
        "\n",
        "# --- ROBUST PATH DETECTION ---\n",
        "possible_paths = [Path(\"data\"), Path(\"../data\"), Path(\".\")]\n",
        "DATA_DIR = None\n",
        "for path in possible_paths:\n",
        "    if (path / \"Tourism_dataset.csv\").exists():\n",
        "        DATA_DIR = path\n",
        "        break\n",
        "\n",
        "if DATA_DIR is None:\n",
        "    sys.exit(\"âŒ Error: Could not find data files.\")\n",
        "# -----------------------------\n",
        "\n",
        "enriched40 = pd.read_csv(DATA_DIR / \"Tourism_dataset.csv\", low_memory=False)\n",
        "tripadvisor_enriched = pd.read_csv(DATA_DIR / \"tripadvisor_enriched_merged.csv\", low_memory=False)\n",
        "reviews = pd.read_csv(DATA_DIR / \"Reviews.csv\", low_memory=False, encoding='latin1')\n",
        "\n",
        "if \"Location_Name\" in reviews.columns:\n",
        "    reviews = reviews.rename(columns={\"Location_Name\": \"place_name\"})\n",
        "reviews[\"place_name\"] = reviews[\"place_name\"].astype(str).str.strip().str.lower()\n",
        "enriched40[\"name_clean\"] = enriched40[\"name\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "canonical_names = enriched40[\"name_clean\"].tolist()\n",
        "reviews[\"matched_name\"] = reviews[\"place_name\"].apply(lambda x: (difflib.get_close_matches(x, canonical_names, n=1, cutoff=0.75) or [x])[0])\n",
        "\n",
        "agg_reviews = reviews.groupby(\"matched_name\").agg(\n",
        "    avg_rating=(\"Rating\", \"mean\"),\n",
        "    review_count=(\"Rating\", \"count\")\n",
        ").reset_index().rename(columns={\"matched_name\": \"place_name\"})\n",
        "\n",
        "def get_sentiment(text):\n",
        "    try:\n",
        "        return TextBlob(str(text)).sentiment.polarity\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "if \"Review\" in tripadvisor_enriched.columns:\n",
        "    tripadvisor_enriched[\"sentiment\"] = tripadvisor_enriched[\"Review\"].apply(get_sentiment)\n",
        "    agg_sent = tripadvisor_enriched.groupby(\"place_name\").agg(\n",
        "        avg_sentiment=(\"sentiment\", \"mean\")\n",
        "    ).reset_index()\n",
        "else:\n",
        "    agg_sent = pd.DataFrame(columns=[\"place_name\", \"avg_sentiment\"])\n",
        "\n",
        "master = enriched40.rename(columns={\"name_clean\": \"place_name\"})\n",
        "master = master.merge(agg_reviews, on=\"place_name\", how=\"left\")\n",
        "master = master.merge(agg_sent, on=\"place_name\", how=\"left\")\n",
        "\n",
        "master[\"avg_rating\"] = master[\"avg_rating\"].fillna(master[\"avg_rating\"].mean())\n",
        "master[\"review_count\"] = master[\"review_count\"].fillna(0)\n",
        "master[\"avg_sentiment\"] = master[\"avg_sentiment\"].fillna(0.0)\n",
        "\n",
        "max_ven = master[\"vendor_count\"].max() or 1\n",
        "max_rev = master[\"review_count\"].max() or 1\n",
        "\n",
        "master[\"local_empowerment_signal\"] = (\n",
        "    0.4 * (master[\"vendor_count\"] / max_ven) +\n",
        "    0.4 * (master[\"review_count\"] / max_rev) +\n",
        "    0.2 * (pd.to_numeric(master[\"local_empowerment_index\"], errors='coerce').fillna(0) / 10.0)\n",
        ")\n",
        "\n",
        "master.to_csv(DATA_DIR / \"master_enriched_dataset.csv\", index=False)\n",
        "print(\"âœ… Master dataset saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKuaAgKsLra-",
        "outputId": "5cf30a69-16a7-495d-cec7-ac4ef96306e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from ..\\data...\n",
            "ðŸ¤– Training & Evaluating models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Projects\\Projects Collection\\Final Year Project\\Tourism Application\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:813: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Random Forest CV Mean: 90.95% | Test Acc: 87.50%\n",
            "   Logistic Regression CV Mean: 90.95% | Test Acc: 75.00%\n",
            "ðŸ† Best Model Selected: Random Forest\n",
            "âœ… Models, XAI background data, and metrics saved to models\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Projects\\Projects Collection\\Final Year Project\\Tourism Application\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:813: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import joblib\n",
        "import sys\n",
        "\n",
        "# --- ROBUST PATH DETECTION ---\n",
        "possible_paths = [Path(\"data\"), Path(\"../data\"), Path(\".\")]\n",
        "DATA_DIR = None\n",
        "for path in possible_paths:\n",
        "    if (path / \"master_enriched_dataset.csv\").exists():\n",
        "        DATA_DIR = path\n",
        "        break\n",
        "if DATA_DIR is None:\n",
        "    # Fallback checks\n",
        "    if Path(\"master_enriched_dataset.csv\").exists():\n",
        "        DATA_DIR = Path(\".\")\n",
        "    else:\n",
        "        sys.exit(\"âŒ Error: master_enriched_dataset.csv not found. Run step 04 first.\")\n",
        "# -----------------------------\n",
        "\n",
        "MODEL_DIR = Path(\"models\")\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Loading data from {DATA_DIR}...\")\n",
        "df = pd.read_csv(DATA_DIR / \"master_enriched_dataset.csv\")\n",
        "\n",
        "# --- Step 2: Preprocessing ---\n",
        "# Handle missing values & Target\n",
        "df = df.dropna(subset=[\"category\"])\n",
        "df[\"category\"] = df[\"category\"].str.lower().str.strip()\n",
        "df[\"category\"] = df[\"category\"].replace({\"hperadeniya eco\": \"eco\"})\n",
        "\n",
        "features = [\n",
        "    \"eco_score\", \"cultural_score\", \"vendor_count\", \"local_empowerment_index\",\n",
        "    \"accessibility_score\", \"danger_level\", \"avg_rating\", \"review_count\", \"avg_sentiment\"\n",
        "]\n",
        "\n",
        "# Map crowd level to numeric\n",
        "df[\"crowd_level_num\"] = df[\"crowd_level\"].map({\"low\":1, \"medium\":2, \"high\":3}).fillna(2)\n",
        "features.append(\"crowd_level_num\")\n",
        "\n",
        "# Fill missing numeric values\n",
        "for f in features:\n",
        "    df[f] = pd.to_numeric(df[f], errors=\"coerce\").fillna(df[f].median())\n",
        "\n",
        "X = df[features]\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[\"category\"])\n",
        "\n",
        "# Split Data\n",
        "# Using stratify to maintain class balance\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Scale Data\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "# --- Step 3: Model Training & Comparison ---\n",
        "print(\"ðŸ¤– Training & Evaluating models...\")\n",
        "\n",
        "# 5-Fold Cross Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Algorithm 1: Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_cv_scores = cross_val_score(rf, X_train_s, y_train, cv=cv, scoring='accuracy')\n",
        "rf.fit(X_train_s, y_train)\n",
        "rf_pred = rf.predict(X_test_s)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "print(f\"   Random Forest CV Mean: {rf_cv_scores.mean():.2%} | Test Acc: {rf_acc:.2%}\")\n",
        "\n",
        "# Algorithm 2: Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_cv_scores = cross_val_score(lr, X_train_s, y_train, cv=cv, scoring='accuracy')\n",
        "lr.fit(X_train_s, y_train)\n",
        "lr_pred = lr.predict(X_test_s)\n",
        "lr_acc = accuracy_score(y_test, lr_pred)\n",
        "print(f\"   Logistic Regression CV Mean: {lr_cv_scores.mean():.2%} | Test Acc: {lr_acc:.2%}\")\n",
        "\n",
        "# Select Best Model\n",
        "if rf_acc >= lr_acc:\n",
        "    best_model = rf\n",
        "    best_name = \"Random Forest\"\n",
        "    best_acc = rf_acc\n",
        "    final_preds = rf_pred\n",
        "else:\n",
        "    best_model = lr\n",
        "    best_name = \"Logistic Regression\"\n",
        "    best_acc = lr_acc\n",
        "    final_preds = lr_pred\n",
        "\n",
        "print(f\"ðŸ† Best Model Selected: {best_name}\")\n",
        "\n",
        "# Calculate Confusion Matrix for the best model\n",
        "cm = confusion_matrix(y_test, final_preds)\n",
        "\n",
        "# --- Step 4: Explainable AI Preparation ---\n",
        "# Save a background dataset for SHAP (important for the app!)\n",
        "# We take a sample of the scaled training data (e.g., 100 rows)\n",
        "# This makes the explainer efficient.\n",
        "if X_train_s.shape[0] > 100:\n",
        "    background_indices = np.random.choice(X_train_s.shape[0], 100, replace=False)\n",
        "    background_data = X_train_s[background_indices]\n",
        "else:\n",
        "    background_data = X_train_s\n",
        "\n",
        "# --- Save Everything for the App ---\n",
        "joblib.dump(best_model, MODEL_DIR / \"best_model.pkl\")\n",
        "joblib.dump(scaler, MODEL_DIR / \"scaler.pkl\")\n",
        "joblib.dump(le, MODEL_DIR / \"label_encoder.pkl\")\n",
        "joblib.dump(background_data, MODEL_DIR / \"background_data.pkl\") # Saved for SHAP\n",
        "\n",
        "# Save metrics dict for the \"Model Performance\" section\n",
        "metrics = {\n",
        "    \"rf_accuracy\": rf_acc,\n",
        "    \"lr_accuracy\": lr_acc,\n",
        "    \"rf_cv_score\": rf_cv_scores.mean(),\n",
        "    \"lr_cv_score\": lr_cv_scores.mean(),\n",
        "    \"best_model_name\": best_name,\n",
        "    \"confusion_matrix\": cm,\n",
        "    \"classes\": le.classes_\n",
        "}\n",
        "joblib.dump(metrics, MODEL_DIR / \"model_metrics.pkl\")\n",
        "\n",
        "print(f\"âœ… Models, XAI background data, and metrics saved to {MODEL_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
